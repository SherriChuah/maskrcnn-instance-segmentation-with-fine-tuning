{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "980c14fe",
      "metadata": {
        "id": "980c14fe"
      },
      "source": [
        "## Testing work"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "LB7I2Mtpsd3r"
      },
      "id": "LB7I2Mtpsd3r",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess FoodSeg103"
      ],
      "metadata": {
        "id": "uJA_3HHnYFXB"
      },
      "id": "uJA_3HHnYFXB"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import base64\n",
        "import zlib\n",
        "import cv2\n",
        "\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "def get_class_and_tag_id_mapping(meta_dir: str):\n",
        "    with open(meta_dir, 'r') as file:\n",
        "        meta_data = json.load(file)\n",
        "\n",
        "    class_mapping = {}\n",
        "    tag_mapping = {}\n",
        "    count = 1\n",
        "    for meta_cls in meta_data['classes']:\n",
        "        class_mapping[meta_cls[\"id\"]] = {\"id\": count, \"name\": meta_cls[\"title\"]}\n",
        "        count += 1\n",
        "\n",
        "    for meta_tags in meta_data['tags']:\n",
        "        tag_mapping[meta_tags[\"id\"]] = {\"id\": count, \"name\": meta_tags[\"name\"]}\n",
        "        count += 1\n",
        "\n",
        "    return class_mapping, tag_mapping"
      ],
      "metadata": {
        "id": "bUgHVLVy2oMY"
      },
      "id": "bUgHVLVy2oMY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_bitmap(bitmap_data, origin, size):\n",
        "    compressed_data = base64.b64decode(bitmap_data)\n",
        "    decoded = zlib.decompress(compressed_data)\n",
        "    mask_img = Image.open(BytesIO(decoded)).convert(\"L\")\n",
        "    full_mask = Image.new(\"L\", size, 0)\n",
        "    full_mask.paste(mask_img, tuple(origin))\n",
        "    return np.array(full_mask, dtype=np.uint8)\n",
        "\n",
        "def extract_bbox(mask):\n",
        "    pos = np.where(mask)\n",
        "    if pos[0].size == 0 or pos[1].size == 0:\n",
        "        print('No bbox')\n",
        "        return None\n",
        "    xmin = int(np.min(pos[1]))\n",
        "    xmax = int(np.max(pos[1]))\n",
        "    ymin = int(np.min(pos[0]))\n",
        "    ymax = int(np.max(pos[0]))\n",
        "    return [xmin, ymin, xmax - xmin, ymax - ymin]\n",
        "\n",
        "def mask_to_coco_polygons(mask):\n",
        "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    polygons = []\n",
        "    for contour in contours:\n",
        "        contour = contour.flatten().tolist()\n",
        "        if len(contour) >= 6:  # at least 3 points\n",
        "            polygons.append(contour)\n",
        "        else:\n",
        "            print(\"Contour length: \", len(contour))\n",
        "    if not polygons:\n",
        "        print('No polygons')\n",
        "    return polygons\n",
        "\n",
        "\n",
        "def convert_annotations(input_dir, class_id_mapping, tag_id_mapping):\n",
        "    coco = {\n",
        "        \"images\": [],\n",
        "        \"annotations\": [],\n",
        "        \"categories\": []\n",
        "    }\n",
        "\n",
        "    class_id_map = {}\n",
        "    annotation_id = 1\n",
        "\n",
        "    for filename in tqdm(os.listdir(input_dir + '/ann')):\n",
        "        if not filename.endswith(\".json\"):\n",
        "            continue\n",
        "\n",
        "        ann_path = os.path.join(input_dir, f'ann/{filename}')\n",
        "        image_path = os.path.join(input_dir, f'img/{filename.replace(\".json\", \"\")}')\n",
        "\n",
        "        if not os.path.exists(image_path):\n",
        "            continue\n",
        "\n",
        "        with open(ann_path) as f:\n",
        "            ann_data = json.load(f)\n",
        "\n",
        "        width = ann_data[\"size\"][\"width\"]\n",
        "        height = ann_data[\"size\"][\"height\"]\n",
        "\n",
        "        coco[\"images\"].append({\n",
        "            \"id\": int(filename.replace(\".jpg\", \"\").replace(\".json\", \"\")),\n",
        "            \"file_name\": filename.replace(\".json\", \"\"),\n",
        "            \"width\": width,\n",
        "            \"height\": height\n",
        "        })\n",
        "\n",
        "        for obj in ann_data.get(\"objects\", []):\n",
        "            if obj[\"geometryType\"] != \"bitmap\":\n",
        "                continue\n",
        "\n",
        "            category_id = class_id_mapping[obj[\"classId\"]][\"id\"]\n",
        "\n",
        "            bitmap = obj[\"bitmap\"]\n",
        "            mask = decode_bitmap(bitmap[\"data\"], bitmap[\"origin\"], (width, height))\n",
        "            bbox = extract_bbox(mask)\n",
        "            polygons = mask_to_coco_polygons(mask)\n",
        "\n",
        "            if polygons == []:\n",
        "                continue\n",
        "\n",
        "            coco[\"annotations\"].append({\n",
        "                \"id\": annotation_id,\n",
        "                \"image_id\": int(filename.replace(\".json\", \"\").replace(\".jpg\", \"\")),\n",
        "                \"category_id\": category_id,\n",
        "                \"bbox\": bbox,\n",
        "                \"area\": int(np.sum(mask > 0)),\n",
        "                \"iscrowd\": 0,\n",
        "                \"segmentation\": polygons\n",
        "            })\n",
        "\n",
        "            annotation_id += 1\n",
        "\n",
        "            # Write categories if not there\n",
        "\n",
        "            curr_cat = {\n",
        "                \"id\": category_id,\n",
        "                \"name\": class_id_mapping[obj[\"classId\"]][\"name\"],\n",
        "                \"supercategory_id\": tag_id_mapping[obj[\"tags\"][0][\"tagId\"]][\"id\"],\n",
        "                \"supercategory\": tag_id_mapping[obj[\"tags\"][0][\"tagId\"]][\"name\"]\n",
        "            }\n",
        "\n",
        "            if curr_cat not in coco[\"categories\"]:\n",
        "              coco[\"categories\"].append(curr_cat)\n",
        "\n",
        "    return coco\n",
        "\n",
        "\n",
        "# Main\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    meta_dir = \"/content/drive/MyDrive/foodseg103/meta.json\"\n",
        "\n",
        "    class_id_mapping, tag_id_mapping = get_class_and_tag_id_mapping(meta_dir)\n",
        "\n",
        "    train_input_dir = \"/content/drive/MyDrive/foodseg103/train\"\n",
        "    test_input_dir = \"/content/drive/MyDrive/foodseg103/test\"\n",
        "\n",
        "    train_output_path = \"/content/drive/MyDrive/foodseg103/train.json\"\n",
        "    test_output_path = \"/content/drive/MyDrive/foodseg103/test.json\"\n",
        "\n",
        "    train_formatted_ann_data = convert_annotations(train_input_dir, class_id_mapping, tag_id_mapping)\n",
        "    test_formatted_ann_data = convert_annotations(test_input_dir, class_id_mapping, tag_id_mapping)\n",
        "\n",
        "    with open(train_output_path, \"w\") as f:\n",
        "        json.dump(train_formatted_ann_data, f)\n",
        "\n",
        "    with open(test_output_path, \"w\") as f:\n",
        "        json.dump(test_formatted_ann_data, f)\n",
        "\n",
        "    print(f\"âœ… COCO annotations saved to: {train_output_path} and {test_output_path}\")"
      ],
      "metadata": {
        "id": "VaUmJ23JYJPd"
      },
      "id": "VaUmJ23JYJPd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "meta_dir = \"/content/drive/MyDrive/foodseg103/meta.json\"\n",
        "class_id_mapping, tag_id_mapping = get_class_and_tag_id_mapping(meta_dir)"
      ],
      "metadata": {
        "id": "vduyW8gu2k2Q"
      },
      "id": "vduyW8gu2k2Q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from matplotlib.patches import Polygon\n",
        "from PIL import Image\n",
        "\n",
        "def draw_image_with_boxes(image_path, annotations, categories):\n",
        "    \"\"\"\n",
        "    Draws bounding boxes and category labels from COCO-style annotations.\n",
        "\n",
        "    Args:\n",
        "        image_path (str): Path to the image.\n",
        "        annotations (list): List of annotations with:\n",
        "            - bbox: [x, y, width, height]\n",
        "            - category_id: int\n",
        "            - segmentation: list of polygons (each polygon is a list of x,y coords)\n",
        "            - image_id: int\n",
        "        categories (list): List of dicts with 'id' and 'name' fields.\n",
        "                           e.g., [{\"id\": 1, \"name\": \"rice\"}, ...]\n",
        "    \"\"\"\n",
        "    # Build a mapping from category_id to name\n",
        "    category_id_to_name = {cat[\"id\"]: cat[\"name\"] for cat in categories}\n",
        "\n",
        "    # Load image\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    filename = os.path.splitext(os.path.basename(image_path))[0]\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.imshow(image)\n",
        "    ax = plt.gca()\n",
        "\n",
        "    for ann in annotations:\n",
        "        if ann[\"image_id\"] != int(filename):\n",
        "          continue\n",
        "        bbox = ann[\"bbox\"]\n",
        "        category_id = ann[\"category_id\"]\n",
        "        label = category_id_to_name.get(category_id, f\"ID {category_id}\")\n",
        "\n",
        "        # Draw rectangle\n",
        "        rect = patches.Rectangle(\n",
        "            (bbox[0], bbox[1]), bbox[2], bbox[3],\n",
        "            linewidth=2, edgecolor='red', facecolor='none'\n",
        "        )\n",
        "        ax.add_patch(rect)\n",
        "\n",
        "        # Draw label\n",
        "        ax.text(bbox[0], bbox[1] - 5, label,\n",
        "                color='white', backgroundcolor='red', fontsize=10)\n",
        "\n",
        "        # Draw polygon masks\n",
        "        if \"segmentation\" in ann:\n",
        "            segmentations = ann[\"segmentation\"]\n",
        "            # COCO segmentation can be a list of polygons or a single polygon list\n",
        "            # Ensure it's a list of polygons\n",
        "            print(len(segmentations))\n",
        "            print(segmentations)\n",
        "            if isinstance(segmentations[0], list):\n",
        "                polygons = segmentations\n",
        "            else:\n",
        "                polygons = [segmentations]\n",
        "\n",
        "            for poly in polygons:\n",
        "                # poly is a flat list: [x1, y1, x2, y2, ..., xn, yn]\n",
        "                # Convert to Nx2 array of points\n",
        "                poly_points = [(poly[i], poly[i+1]) for i in range(0, len(poly), 2)]\n",
        "                polygon_patch = Polygon(poly_points, closed=True, linewidth=1,\n",
        "                                        edgecolor='yellow', facecolor='yellow', alpha=0.4)\n",
        "                ax.add_patch(polygon_patch)\n",
        "\n",
        "    plt.axis(\"off\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "SiPpd5_Hjahv"
      },
      "id": "SiPpd5_Hjahv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/foodseg103/train.json\", 'r') as f:\n",
        "    train_formatted_ann_data = json.load(f)"
      ],
      "metadata": {
        "id": "C9zJxxzfGGRp"
      },
      "id": "C9zJxxzfGGRp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "draw_image_with_boxes(\n",
        "    \"/content/drive/MyDrive/foodseg103/train/img/00001080.jpg\",\n",
        "    train_formatted_ann_data[\"annotations\"],\n",
        "    train_formatted_ann_data[\"categories\"])"
      ],
      "metadata": {
        "id": "ZFkPFBKPk9VR"
      },
      "id": "ZFkPFBKPk9VR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Official code part"
      ],
      "metadata": {
        "id": "8GACHxb_pVmH"
      },
      "id": "8GACHxb_pVmH"
    },
    {
      "cell_type": "markdown",
      "id": "ac2af55a",
      "metadata": {
        "id": "ac2af55a"
      },
      "source": [
        "#### Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd408dda",
      "metadata": {
        "id": "cd408dda"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import torchvision.transforms.functional as F\n",
        "\n",
        "class Compose:\n",
        "    def __init__(self, transforms):\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __call__(self, image, target):\n",
        "        for t in self.transforms:\n",
        "            image, target = t(image, target)\n",
        "        return image, target\n",
        "\n",
        "class RandomHorizontalFlip:\n",
        "    def __init__(self, prob=0.5):\n",
        "        self.prob = prob\n",
        "\n",
        "    def __call__(self, image, target):\n",
        "        if random.random() < self.prob:\n",
        "            # image must be a PIL image here\n",
        "            image = F.hflip(image)\n",
        "\n",
        "            width, _ = image.size  # PIL image size\n",
        "\n",
        "            # Flip boxes\n",
        "            if \"boxes\" in target:\n",
        "                boxes = target[\"boxes\"]\n",
        "                boxes[:, [0, 2]] = width - boxes[:, [2, 0]]\n",
        "                target[\"boxes\"] = boxes\n",
        "\n",
        "            # Flip masks (tensor)\n",
        "            if \"masks\" in target:\n",
        "                target[\"masks\"] = target[\"masks\"].flip(-1)\n",
        "\n",
        "        return image, target\n",
        "\n",
        "class ToTensor:\n",
        "    def __call__(self, image, target):\n",
        "        image = F.to_tensor(image)  # converts PIL image to tensor\n",
        "        return image, target\n",
        "\n",
        "def get_transform(train=True):\n",
        "    transforms = []\n",
        "    if train:\n",
        "        transforms.append(RandomHorizontalFlip(0.5))\n",
        "    transforms.append(ToTensor())\n",
        "    return Compose(transforms)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "780481f9",
      "metadata": {
        "id": "780481f9"
      },
      "source": [
        "#### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6898d632",
      "metadata": {
        "id": "6898d632"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "from PIL import Image, ImageDraw\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms.functional as F\n",
        "from pycocotools.coco import COCO\n",
        "\n",
        "class FoodSegJSONDataset(Dataset):\n",
        "    def __init__(self, img_dir, ann_path, transforms=None):\n",
        "        self.image_dir = os.path.join(img_dir, \"img\")\n",
        "        self.coco = COCO(ann_path)\n",
        "        self.image_ids = list(self.coco.imgs.keys())\n",
        "        self.transforms = transforms\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_ids)\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_id = self.image_ids[idx]\n",
        "        img_info = self.coco.loadImgs(image_id)[0]\n",
        "        img_path = os.path.join(self.image_dir, img_info['file_name'])\n",
        "\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        ann_ids = self.coco.getAnnIds(imgIds=image_id)\n",
        "        anns = self.coco.loadAnns(ann_ids)\n",
        "\n",
        "        boxes = []\n",
        "        labels = []\n",
        "        masks = []\n",
        "\n",
        "        for ann in anns:\n",
        "            if ('segmentation' not in ann) or ('bbox' not in ann):\n",
        "                continue\n",
        "\n",
        "            masks.append(self.coco.annToMask(ann))\n",
        "            boxes.append(ann['bbox'])\n",
        "            labels.append(ann['category_id'])\n",
        "\n",
        "        if len(masks) == 0:\n",
        "            return self.__getitem__((idx + 1) % len(self))  # skip bad data\n",
        "\n",
        "        masks = torch.as_tensor(np.array(masks), dtype=torch.uint8)\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
        "\n",
        "        # COCO bbox is [x, y, width, height] -> convert to [x1, y1, x2, y2]\n",
        "        boxes[:, 2] = boxes[:, 0] + boxes[:, 2]\n",
        "        boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n",
        "\n",
        "        target = {\n",
        "            \"boxes\": boxes,\n",
        "            \"labels\": labels,\n",
        "            \"masks\": masks,\n",
        "            \"image_id\": torch.tensor([image_id]),\n",
        "            \"area\": torch.tensor([ann[\"area\"] for ann in anns], dtype=torch.float32),\n",
        "            \"iscrowd\": torch.tensor([ann.get(\"iscrowd\", 0) for ann in anns], dtype=torch.int64)\n",
        "        }\n",
        "\n",
        "        if self.transforms:\n",
        "            img, target = self.transforms(img, target)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ea6e477",
      "metadata": {
        "id": "7ea6e477"
      },
      "source": [
        "#### Engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e77c167c",
      "metadata": {
        "id": "e77c167c"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def train_one_epoch(model, optimiser, data_loader, device, epoch):\n",
        "    model.train()\n",
        "    google_drive_path = '/content/drive/MyDrive/foodseg103'\n",
        "\n",
        "    count = 0\n",
        "    for images, targets in tqdm(data_loader, desc=f\"Epoch {epoch}\"):\n",
        "        images = list(img.to(device) for img in images)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        loss_dict = model(images, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "        optimiser.zero_grad()\n",
        "        losses.backward()\n",
        "        optimiser.step()\n",
        "\n",
        "        if count%10 == 0:\n",
        "          os.makedirs(google_drive_path + \"/outputs/models\", exist_ok=True)\n",
        "          torch.save(model.state_dict(), f\"{google_drive_path}/outputs/models/model_epoch_{epoch}_checkpoint_{count}.pth\")\n",
        "\n",
        "        count += 1\n",
        "\n",
        "\n",
        "\n",
        "    print(f\"Loss: {losses.item():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b979e9e8",
      "metadata": {
        "id": "b979e9e8"
      },
      "source": [
        "#### Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "272b0254",
      "metadata": {
        "id": "272b0254"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "\n",
        "from torchvision.models.detection import (\n",
        "    maskrcnn_resnet50_fpn_v2,\n",
        "    MaskRCNN_ResNet50_FPN_V2_Weights,\n",
        "    faster_rcnn,\n",
        "    mask_rcnn)\n",
        "\n",
        "\n",
        "def get_model(num_classes):\n",
        "    model = maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.COCO_V1)\n",
        "\n",
        "    # print(model.roi_heads)\n",
        "    # print()\n",
        "    # print()\n",
        "    # print()\n",
        "\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    model.roi_heads.box_predictor = faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
        "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
        "\n",
        "    hidden_layer = 256\n",
        "    model.roi_heads.mask_predictor = mask_rcnn.MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes)\n",
        "\n",
        "    # print(model.roi_heads)\n",
        "    # print()\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fa0fc45",
      "metadata": {
        "id": "0fa0fc45"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "\n",
        "def show_image_with_masks(img, pred, categories=None, score_thresh=0.5):\n",
        "    img = img.permute(1,2,0).numpy()\n",
        "\n",
        "    plt.figure(figsize=(10,10))\n",
        "    plt.imshow(img)\n",
        "\n",
        "    ax = plt.gca()\n",
        "\n",
        "    masks = pred[\"masks\"]\n",
        "    boxes = pred[\"boxes\"]\n",
        "    labels = pred[\"labels\"]\n",
        "    scores = pred[\"scores\"]\n",
        "\n",
        "    for i in range(len(masks)):\n",
        "        if scores[i] < score_thresh:\n",
        "            continue\n",
        "\n",
        "        mask = masks[i,0].mul(255).byte().cpu().numpy()\n",
        "        color = np.random.rand(3,)\n",
        "\n",
        "        ax.contour(mask, levels=[0.5], colors=[color])\n",
        "\n",
        "        x1, y1, x2, y2 = boxes[i].detach().cpu().numpy()\n",
        "        ax.add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1,\n",
        "                                   fill=False, color=color, linewidth=2))\n",
        "        label_id = labels[i].item()\n",
        "        label_name = categories[label_id] if categories and label_id in categories else str(label_id)\n",
        "        ax.text(x1, y1, f\"{label_id}:{scores[i]:.2f}\", color=color, fontsize=12,\n",
        "                bbox=dict(facecolor='white', edgecolor=color, boxstyle='round,pad=0.2'))\n",
        "\n",
        "    plt.axis(\"off\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9f47044",
      "metadata": {
        "id": "b9f47044"
      },
      "source": [
        "#### Main.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# need class_id_mapping mapping"
      ],
      "metadata": {
        "id": "F8CEzpUlr3ZZ"
      },
      "id": "F8CEzpUlr3ZZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "586c4de6",
      "metadata": {
        "id": "586c4de6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import json\n",
        "import os\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# from dataset.foodseg_json_dataset import FoodSegJSONDataset\n",
        "# from models.mask_rcnn import get_model\n",
        "# from utils.transforms import get_transform\n",
        "# from engine.train import train_one_epoch\n",
        "# from utils.visualise import show_image_with_masks\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))\n",
        "\n",
        "\n",
        "def load_categories(meta_path):\n",
        "    with open(meta_path) as f:\n",
        "        meta = json.load(f)\n",
        "    return {cat['id']: cat['title'] for cat in meta['classes']}\n",
        "\n",
        "\n",
        "def main():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"device: {device}\")\n",
        "\n",
        "    # train_data_path = \"data/foodseg103/train\"\n",
        "    # test_data_path = \"data/foodseg103/test\"\n",
        "    # meta_path = \"data/foodseg103/meta.json\"\n",
        "\n",
        "    google_drive_path = '/content/drive/MyDrive/foodseg103'\n",
        "\n",
        "    train_data_path = os.path.join(google_drive_path, 'train')\n",
        "    test_data_path = os.path.join(google_drive_path, 'test')\n",
        "    meta_path = os.path.join(google_drive_path, 'meta.json')\n",
        "\n",
        "    train_ann_path = os.path.join(google_drive_path, 'train.json')\n",
        "    test_ann_path = os.path.join(google_drive_path, 'test.json')\n",
        "\n",
        "    # Dataset and Dataloader\n",
        "    dataset = FoodSegJSONDataset(train_data_path, train_ann_path, transforms=get_transform(train=True))\n",
        "    data_loader = DataLoader(dataset, batch_size=3, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "    print(\"len\", len(dataset))\n",
        "\n",
        "    test_dataset = FoodSegJSONDataset(test_data_path, test_ann_path, transforms=get_transform(train=False))\n",
        "    # test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "    # Model\n",
        "    num_classes = len(class_id_mapping.keys()) + 1 # background + category count\n",
        "    print()\n",
        "    print(f\"num classes: {num_classes}\")\n",
        "    model = get_model(num_classes).to(device)\n",
        "\n",
        "    # Optimizer\n",
        "    params = [p for p in model.parameters() if p.requires_grad]\n",
        "    optimiser = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
        "\n",
        "    # Training Loop\n",
        "    for epoch in range(1):\n",
        "        train_one_epoch(model, optimiser, data_loader, device, epoch)\n",
        "\n",
        "    # Visualize predictions on test set\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        img, _ = test_dataset[0]\n",
        "        pred = model([img.to(device)])[0]\n",
        "\n",
        "    show_image_with_masks(img, pred, class_id_mapping)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "id": "BMhlWFi-rS1v"
      },
      "id": "BMhlWFi-rS1v",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IKIzrZ2hAFcT"
      },
      "id": "IKIzrZ2hAFcT",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}