{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "980c14fe",
   "metadata": {},
   "source": [
    "## Testing work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780481f9",
   "metadata": {},
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6898d632",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "class FoodSegJSONDataset(Dataset):\n",
    "    def __init__(self, root_dir, transforms=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.img_dir = os.path.join(root_dir, \"img\")\n",
    "        self.ann_dir = os.path.join(root_dir, \"ann\")\n",
    "        self.transforms = transforms\n",
    "\n",
    "        self.img_files = sorted([\n",
    "            f for f in os.listdir(self.img_dir)\n",
    "            if f.endswith('.jpg')\n",
    "        ])\n",
    "\n",
    "    def load_annotation(self, ann_path, image_size):\n",
    "        with open(ann_path) as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        masks = []\n",
    "        boxes = []\n",
    "        labels = []\n",
    "\n",
    "        for obj in data['objects']:\n",
    "            label_id = obj['category_id']\n",
    "            polygons = obj['segmentation']\n",
    "\n",
    "            # Draw mask from polygons\n",
    "            mask = Image.new(\"L\", image_size, 0)\n",
    "            for poly in polygons:\n",
    "                ImageDraw.Draw(mask).polygon(poly, outline=1, fill=1)\n",
    "\n",
    "            mask_np = np.array(mask, dtype=np.uint8)\n",
    "            pos = np.where(mask_np)\n",
    "            if pos[0].size == 0 or pos[1].size == 0:\n",
    "                continue\n",
    "\n",
    "            xmin = np.min(pos[1])\n",
    "            xmax = np.max(pos[1])\n",
    "            ymin = np.min(pos[0])\n",
    "            ymax = np.max(pos[0])\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "            masks.append(mask_np)\n",
    "            labels.append(label_id)\n",
    "\n",
    "        if not masks:\n",
    "            return None\n",
    "\n",
    "        masks = torch.as_tensor(np.stack(masks), dtype=torch.uint8)\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        return {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels,\n",
    "            \"masks\": masks,\n",
    "            \"area\": (masks.sum(dim=(1, 2))).float(),\n",
    "            \"iscrowd\": torch.zeros((len(labels),), dtype=torch.int64),\n",
    "        }\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.img_files[idx]\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        ann_path = os.path.join(self.ann_dir, img_name.replace('.jpg', '.json'))\n",
    "\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        width, height = img.size\n",
    "\n",
    "        target = self.load_annotation(ann_path, (width, height))\n",
    "        if target is None:\n",
    "            return self.__getitem__((idx + 1) % len(self))  # skip bad mask\n",
    "\n",
    "        target[\"image_id\"] = torch.tensor([idx])\n",
    "\n",
    "        if self.transforms:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        img = F.to_tensor(img)\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea6e477",
   "metadata": {},
   "source": [
    "#### Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77c167c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def train_one_epoch(model, optimiser, data_loader, device, epoch):\n",
    "    model.train()\n",
    "\n",
    "    for images, targets in tqdm(data_loader, desc=f\"Epoch {epoch}\"):\n",
    "        images = list(img.to(device) for img in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        optimiser.zero_grad()\n",
    "        losses.backward()\n",
    "        optimiser.step()\n",
    "    \n",
    "    print(f\"Loss: {losses.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b979e9e8",
   "metadata": {},
   "source": [
    "#### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272b0254",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "from torchvision.models.detection import (\n",
    "    maskrcnn_resnet50_fpn_v2, \n",
    "    MaskRCNN_ResNet50_FPN_V2_Weights,\n",
    "    faster_rcnn,\n",
    "    mask_rcnn)\n",
    "\n",
    "\n",
    "def get_model(num_classes):\n",
    "    model = maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.COCO_V1)\n",
    "\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
    "    in_features_mask = model.roi_heads.mask_predictor.con5_mask.in_channels\n",
    "\n",
    "    hidden_layer = 256\n",
    "    model.roi_heads.mask_predictor = mask_rcnn.MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2af55a",
   "metadata": {},
   "source": [
    "#### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd408dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "def get_transform(train=True):\n",
    "    transforms = []\n",
    "    # convert image to pytorch tensor\n",
    "    transforms.append(T.ToTensor())\n",
    "\n",
    "    if train:\n",
    "        # apply horizontal flip randomly\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    \n",
    "    # combined all transforms into a pipeline\n",
    "    return T.Compose(transforms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa0fc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "\n",
    "def show_image_with_masks(img, pred, categories=None, score_thresh=0.5):\n",
    "    img = img.permute(1,2,0).numpy()\n",
    "    \n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.imshow(img)\n",
    "\n",
    "    ax = plt.gca()\n",
    "\n",
    "    masks = pred[\"masks\"]\n",
    "    boxes = pred[\"boxes\"]\n",
    "    labels = pred[\"labels\"]\n",
    "    scores = pred[\"scores\"]\n",
    "\n",
    "    for i in range(len(masks)):\n",
    "        if scores[i] < score_thresh:\n",
    "            continue\n",
    "\n",
    "        mask = masks[i,0].mul(255).byte().cpu().numpy()\n",
    "        color = np.random.rand(3,)\n",
    "        \n",
    "        ax.contour(mask, levels=[0.5], colors=[color])\n",
    "\n",
    "        x1, y1, x2, y2 = boxes[i].detach().cpu().numpy()\n",
    "        ax.add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1,\n",
    "                                   fill=False, color=color, linewidth=2))\n",
    "        label_id = labels[i].item()\n",
    "        label_name = categories[label_id] if categories and label_id in categories else str(label_id)\n",
    "        ax.text(x1, y1, f\"{label_id}:{scores[i]:.2f}\", color=color, fontsize=12,\n",
    "                bbox=dict(facecolor='white', edgecolor=color, boxstyle='round,pad=0.2'))\n",
    "    \n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f47044",
   "metadata": {},
   "source": [
    "#### Main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586c4de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import os\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from dataset.foodseg_json_dataset import FoodSegJSONDataset\n",
    "from models.mask_rcnn import get_model\n",
    "from utils.transforms import get_transform\n",
    "from engine.train import train_one_epoch\n",
    "from utils.visualise import show_image_with_masks\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "\n",
    "def load_categories(meta_path):\n",
    "    with open(meta_path) as f:\n",
    "        meta = json.load(f)\n",
    "    return {cat['id']: cat['title'] for cat in meta['classes']}\n",
    "\n",
    "\n",
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"device: {device}\")\n",
    "\n",
    "    train_data_path = \"data/foodseg103/train\"\n",
    "    test_data_path = \"data/foodseg103/test\"\n",
    "    meta_path = \"data/foodseg103/meta.json\"\n",
    "\n",
    "    categories = load_categories(meta_path)\n",
    "    print(categories)\n",
    "\n",
    "    # Dataset and Dataloader\n",
    "    dataset = FoodSegJSONDataset(train_data_path, transforms=get_transform(train=True))\n",
    "    data_loader = DataLoader(dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "    test_dataset = FoodSegJSONDataset(test_data_path, transforms=get_transform(train=False))\n",
    "    # test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "    # Model\n",
    "    num_classes = max(categories.keys()) + 1 # background + category count\n",
    "    print(f\"num classes: {num_classes}\")\n",
    "    model = get_model(num_classes).to(device)\n",
    "\n",
    "    # Optimizer\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimiser = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "    # Training Loop\n",
    "    for epoch in range(3):\n",
    "        train_one_epoch(model, optimiser, data_loader, device, epoch)\n",
    "\n",
    "        os.makedirs(\"outputs/models\", exist_ok=True)\n",
    "        torch.save(model.state_dict(), f\"outputs/models/model_epoch_{epoch}.pth\")\n",
    "\n",
    "    \n",
    "    # Visualize predictions on test set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        img, _ = test_dataset[0]\n",
    "        pred = model([img.to(device)])[0]\n",
    "    \n",
    "    show_image_with_masks(img, pred, categories)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
